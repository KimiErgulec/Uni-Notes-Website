<!doctype html>
<html lang="it">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <title>Calcolo Numerico 2 - Capitolo 1</title>

    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    <!-- Custom styles -->
    <link rel="stylesheet" href="../style.css">
</head>

<body class="minimal-wrapper">

    <div class="top-bar">
        <div class="top-bar-info">
            <div class="top-bar-name">Kimi Ergulec</div>
            <div class="top-bar-university">Universit√† di Verona</div>
        </div>
        <button id="theme-toggle" class="theme-toggle">
            <span class="theme-toggle-icon">üåô</span>
            <span class="theme-toggle-text">Modalit√† Scura</span>
        </button>
    </div>
    <a href="CalcoloNumerico2.html" class="back-home">‚Üê Torna indietro</a>

    <header>
        <h1>Calcolo Numerico 2</h1>
    </header>

    <article>
        <section>
            <h2>1. Metodi per la risoluzione di Sistemi Lineari</h2>

            <h3>1.1 Introduzione</h3>
            <p class="normal-text">
                Vogliamo risolvere problemi nella forma $$Ax=b$$
                dove $A$ √® una matrice con componenti $a_{ij}$ con $i=1,...,m$ e $j=1,...,n$ perci√≤ $A\in M_{m\times n}(\RR)$, $x$ √® un vettore riga in $\RR^n$ e $b\in \RR^m$. Consideriamo per ora, per semplicit√† $m=n$ ovvero matrici quadrate.
            </p>

            <h3>1.2 Metodi per risolvere $Ax=b$</h3>
            <ol>
                <li>
                    Calcolare l'inversa di $A$ per poi moltiplicarla ambi i lati: 
                    $$Ax=b \quad\Ra\quad A^{-1}Ax=A^{-1}b\quad\Ra\quad x=A^{-1}b$$ 
                    Questo metodo non conviene siccome il comando matlab <code>inv(A)</code> √® lento e costoso
                </li>

                <li>
                    Metodi Diretti, ovvero algoritmi elementari per risolvere matrici diagonali, triangolari, diagonali inferiori, diagonali superiori. Basati sulle fattorizzazioni di A per renderle matrici elementari. Utile per quando dobbiamo risolvere $Ax=b_i$ con tanti $b$ diversi, cos√¨ fattorizziamo $A$ una unica volta e possiamo riutilizzarla per trovare tanti $x_i$ diversi a seconda della $b_i$ usata. Spesso utilizzata per quando $A$ √® piena.
                </li>

                <li>
                    Metodi Iterativi o indiretti: utilizzate per matrici sparse
                    
                    <div class="lemma">
                        <div class="lemma-title">Matrice Sparsa</div>
                        <p>
                            Una matrice sparsa √® una matrice $n\times n$ con quasi tutti elementi nulli, ovvero che ha solo $O(n)$ elementi non nulli (ha senso siccome l'ordine di elementi di una matrice $n \times n$ √® $O(n^2)$).
                        </p>
                    </div>

                    <p>
                        Il metodo parte da un guess iniziale della soluzione ed utilizza una successione di operazioni per trovare una soluzione pi√π accurata del primo guess. Andando avanti cos√¨ si raggiunge una soluzione sempre pi√π vicina a quella reale. Il residuo (la differenza tra la soluzione numerica e quella esatta) tende ad un numero molto piccolo.
                    </p>

                    <div class="lemma">
                        <div class="lemma-title">Idea:</div>
                        <p>
                            Consideriamo una decomposizione di una matrice $A$ tale che $A=M-N$, inoltre sapendo che $\det(A)\neq 0$ vale che per ogni $b\in \RR^n$ vale che 
                            $$Ax=b \sse Mx = Nx+b$$ 
                            Decidiamo di applicare un procedimento iterativo di tipo punto fisso: 
                            $$Mx^{(k+1)}=Nx^{(k)}+b$$
                            Analizziamo che chiamando $B=M^{-1}N$ otteniamo: 
                            $$B=M^{-1}N=M^{-1}\underbrace{(M-A)}_{A=M-N}=I-M^{-1}A$$ 
                            possiamo quindi riscrivere 
                            $$x^{(k+1)}=Bx^{(k)}+M^{-1}b$$ 
                            $B$ √® detta matrice di iterazione
                        </p>
                    </div>
                </li>
            </ol>

            <h3>1.3 Studio delle Convergenze</h3>
            
            <div class="lemma">
                <div class="lemma-title">Definizione</div>
                <p>
                    Un metodo iterativo si dice <b>Convergente</b> se qualunque sia $x^{(0)}$, la successione $x^{(k)}$ √® convergente.
                </p>
            </div>

            <p>
                Tale propriet√† √® legata dagli autovalori della matrice iterativa. Indichiamo con $\bar{x}$ la soluzione esatta di $Ax=b$ e consideriamo $\hat{x}=B\bar{x}+M^{-1}b$ e $x^{(k+1)}=Bx^{(k)}+M^{-1}b$ dove $e^{(k)}=\hat{x}-x^{(k)}$ √® l'errore. Allora 
                $$\Ra\quad e^{(k+1)}=Be^{(k)}$$ 
                $$e^{(k)}=Be^{(k-1)}=B(Be^{(k-2)})=...=B^ke^{(0)}$$ 
                dove $e^{(0)}=\hat{x}-x^{(0)}$, perci√≤ dipende pi√π da $B$ diminuire l'errore che dal guess iniziale (abbiamo inoltre detto che dovrebbe essere valido per ogni guess iniziale).
            </p>
            
            <div class="theorem">
                <div class="theorem-title">Teorema 1</div>
                <p>
                    <b>Condizione necessaria e sufficiente per la convergenza:</b> il metodo iterativo definito dalla matrice $B$ converge per ogni $x^{(0)}$ se e solo se $\rho(B)\lt 1$ dove $\rho$ √® il raggio spettrale della matrice.
                </p>
                
                <div class="lemma">
                    <div class="lemma-title">Definizione Raggio Spettrale</div>
                    <p>
                        Raggio Spettrale di $M$ √® la massima lunghezza, in modulo, degli autovalori di $M$. 
                        $$\rho(M) = \max(|\lambda_i|)\quad \forall i$$
                    </p>
                </div>
            </div>

            <div class="theorem">
                <div class="theorem-title">Teorema 2</div>
                <p>
                    Sia $M\in \CC^{n\times n}$, $\rho(M)$ il suo raggio spettrale, allora 
                    $$\lim_{k\to \infty}M^k=0\sse\rho(M)\lt 1$$
                    L'iterazione di Punto Fisso con matrice $B$ √® convergente se esiste una norma di matrice tale che $||B||\lt 1$
                </p>
            </div>

            <div class="theorem">
                <div class="theorem-title">Teorema 3</div>
                <p>
                    Il metodo di Jacobi converge per $Ax=b$ se $A$ √® una matrice a diagonale dominante per righe in senso stretto, ovvero $$\max_{1\leq i \leq n}\sum_{j=1}^n|a_{ij}|\lt 1\sse\sum^n_{j=1\atop{j\neq i}}|a_{ij}|\lt |a_{ii}|$$ 
                </p>
            </div>

            <h3>1.4 Metodo di Jacobi</h3>

            <p>
                Prendere la riga $i$-esima per isolare $x_i$ in $Ax=b$ dove $x=\begin{bmatrix}x_1\\ \vdots \\ x_n\end{bmatrix}$. Voglio isolare la riga $i$-esima prendendo solo quella: 
                $$\sum_{j=1}^{n}a_{ij}x_j=b_i\Ra a_{ii}x_i+\sum_{j=1\atop j\neq i}^n a_{ij}x_j=b_i$$
                $$x_i=\frac{1}{a_{ii}}(b_i-\sum_{j=1\atop j\neq i}^na_{ij}x_j)$$
                Da questo deduciamo l'iterazione: 
                $$x_{i}^{(k+1)}=\frac{1}{a_{ii}}(b_i-\sum_{j=1\neq i}^{n}a_{ij}x_j^{(k)})$$
                Manipolando ancora, otteniamo: 
                $$a_{ii}x_i^{(k+1)}=b_i-\sum_{j=1}^{i-1}a_{ij}x_j^{(k)}-\sum_{j=i+1}^na_{ij}x_j^{(k)}\quad \forall i$$
                Deduciamo la forma matriciale del metodo: 
                $$Dx^{(k+1)}=b-(L+U)x^{(k)}\Ra x^{(k+1)}=D\inv b-\underbrace{D\inv(L+U)}_Jx^{(k)}$$ 
                Dove $J$ √® la matrice di iterazione del metodo di <em>Jacobi</em>
            </p>
            <p>
            <h3>
                Metodo di Gauss-Seidel
            </h3>
            Jacobi come visto sopra utilizza le iterazioni una alla volta, prima fa una stima e con la stima ne crea una nuova. Una "ottimizzazione" che possiamo implementare facilemnte √® di migliorare le stime mano a mano. Se siamo all $k$-esima iterazione, usiamo l'iterazione $k-1$ per trarre le componenti. Quindi otteniamo $x^{(k)}_1$ con l'iterazione di $x_i^{(k-1)}$. Una volta ottenuto $x_1^{(k)}$, invece che usare nuovamente le componenti della iterata precedente, utilizziamo tutti i dati possibili da ricavare nuovi, per esempio il dato che abbiamo appena trovato $x_1^{(k)}$ ed i dati precedenti $x_i^{(k-1)}$ con $i=1,...,n$. Otteniamo quindi Gauss-Seidel:
            $$x_i^{(k+1)} = \frac{1}{a_{ii}}\left(b_i - \sum_{j=1}^{i-1}a_{ij}x_j^{(k+1)}-\sum_{j=i+1}^n a_{ij}x_j^{(k)}\right)\forall i$$
            La forma matriciale si presenter√† come: $$x^{(k+1)} = -\left(D+L\right)\inv Ux^{(k)}+ (D+L)\inv b$$Dove $-(D+L)\inv U$ √® la matrice $GS$ di iterazione.
            </p>
            <p>
                <h3>
                    Richiamo Teorico di Matrici Definite Positive:
                </h3>
                Matrici Definite positive sono matrici quadrate simmetriche la cui forma quadratica $x^TAx>0 \forall x\neq 0$. Ovvero tutti gli autovalori sono strettamente positivi. Questo significa che esiste una matrice triagnolare inferiore $L$ tale che $A = LL^T$ che si chiama fattorizzazione di Cholesky.
            </p>
            <p>
                <h3>
                    Convergenza del Metodo di Gauss-Seidel
                </h3>
                <div class="theorem">
                <div class="theorem-title">Teorema 4</div>
                <p>
                    Il metodo di Gauss-Seidel, come quello di Jacobi, converge se la matrice di iterazione √® a diagonale dominante per righe in senso stretto.
                </p>
            </div>
            </p>

            <h3>
                Metodo di Sor
            </h3>
            <p>
                Ha un parametro di rilassamento $$x_i^{(k+1)}= (1-\omega)x_i^{(k)}+\frac{\omega}{a_{ii}}(b_i-\sum_{j=1}^{i-1}a_{ij}x_k^{(k+1)}-\sum_{j=i+1}^{n}a_{ij}x_j^{(k)})$$In forma matriciale $$x^{(k+1)}=\underbrace{(\omega L+D)\inv [(1-\omega)D-\omega L]}_{\text{matrice di iterazione }B\omega}x^{(k)}+\omega(\omega L + D)\inv b$$ 
                Il parametro di rilassamento $\omega$ se √® $\omega \lt 1$ si dice di sottorilassamento mentre $\omega \gt 1$ √® detto sovrarilassamento. Pi√π efficente modo per far convergere √® scegliere $\omega$ in modo che minimizza il raggio di convergenza di della matrice di iterazione. $ 0\leq\omega\leq 2$.
            </p>

            <h3>
                Metodo Del Gradiente
            </h3>
            <p>
                $\bar{x}\in \R^n$ √® soluzione d $Ax=b\sse \bar{x}= \min(f(x)) = \frac{1}{2}x^TAx-x^Tb$ $$\begin{cases}\nabla f(\bar{x}) = 0 \sse A\bar{x}-b=0\\ H(\bar{x}) \gt 0 \end{cases}$$
            </p>
            <h4>Problema di Potenze</h4>
            <p>
                $x^{(x)},p^{(x)}$ fissati, vogliamo minimizzare $f(x(\alpha)) = f(x^{(k)}+\alpha p^{(k)})$

                <ol>
                    <li>
                        Per Minimizzare $f(x(\alpha))$ devo imporre che $$0 = \frac{df}{d\alpha} = \frac{\partial f}{\partial x} \cdot \frac{\partial x}{\partial \alpha} = (Ax-b)\cdot p^{(k)}$$Dove $Ax-b$ √® l'errore. Facciamo delle manipolazioni algebriche: $${p^{(k)}}^T(Ax^{(k)}+A\alpha p^{(k)}-b)={p^{(k)}}^T(-r^{(k)}+\alpha A p^{(k)}) = 0$$
                        $$0 = -{p^{(k)}}^T r^{(k)} + \alpha {p^{(k)}}^T A p^{(k)}\Ra \alpha = \frac{{p^{(k)}}^Tr^{(k)}}{{p^{(k)}}^T Ap^{(k)}}$$
                        Quindi posso trovare il membro di $f(x(\alpha))\gt 0 $ calcolando $$x^{(k+1)}- x^{(k)}+ \frac{{p^{(k)}}^Tr^{(k)}}{p^{(k)}}$$operazione pi√π costosa la facciamo solamente una volta per iterazione e risolviamo $v^{(k)}=Ap^{(k)}$
                    </li>
                    <li>
                        Calcolando il residuol $$r^{(k+1)} = b-Ax^{(k+1)} = b-Ax^{(k)}-\alpha^{(k)}Ap^{(k)}= r^{(k)}-\alpha^{(k)} A p^{(k)} = r^{(k)} - \alpha^{(k)}v^{(k)}$$
                    </li>
                    <li>
                        Step 3A: Ma qual √® una buona direzione di ricerca per $p^{(k)}$? Supponiamo che la direzione di massima discesa √® quella che ha segno contrario del gradiente $-\nabla f = r^{(k)}$. Al primo posto posso usare $p^{(k)}=r^{(0)}$
                    </li>
                    <li>
                        Step 3B: Perci√≤ se facessimo sempre questa scelta. Un trucco per accelerare la sua convergenza consiste nello sfruttare non solo l'inf data del gradiente, ma anche dell'uso di vettori coniugati. Infatti, supponiamo che conoscessimo $n$ vettori coniugati, si potr√† scrivere la soluzione $x$ concreta. Allora partendo dalla direzione del residuo che √® la migliore scelta che possiamo fare localmente, la modifichiamo per renderla coniugata, speriamo, rispetto a tutte le riezioni precedenti. Ottendo in tal modo $$p^{(k)}\begin{cases} r^{(0)} & k=0 \\ r^{(k)}+\beta_k p^{(k-1)} & \forall k\geq 1\end{cases}$$
                        Dove $\beta_k$ deve essere scelto affinch√® $${p^{(k)}}^T A p{(k+1)} = 0 \sse {p^{(k-1)}}^TA p^{(k)}=0$$
                        Propriet√† importante che richiameremo pi√π avanti come <b>STELLA</b>. 
                    </li>
                </ol>
                Troviamo $\beta_{k}$: Partiamo da imponendo ${p^{(k)}}^TAp^{(k)} = 0$: $$(r^{(k)}+\beta_{k}p^{(k+1)})^TAp^{(k-1)} = 0$$ $${r^{(k)}}^T A p^{(k-1)}+\beta_k{p^{(k-1)}}^TAp^{(k-1)}=0$$ $$\Ra \beta_k = -\frac{{r^{(k)}}^TAp^{(k-1)}}{{p^{(k-1)}}^T A p^{(k-1)}}\quad k\geq 1$$

                <h3>Propriet√†:</h3>
                <ol>
                    <li>
                        Propriet√† 1: ${r^{(k+1)}}^T p^{(k)} = 0$ $${r^{(k+1)}}^Tp^{(k)} = (r^{(k)}-\alpha^{(k)}Ap^{(k)})^Tp^{(k)} = {r^{(k)}}^Tp^{(k)}- \alpha^{(k)} {p^{(k)}}^T A^T p^{(k)}$$Siccome $A=A^T$ otteniamo: $${r^{(k)}}^T p^{(k)} - \frac{{p^{(k)}}^Tr^{(k)}}{{p^{(k)}}^T A p^{(k)}}{p^{(k)}}^T A p^{(k)} = 0$$
                    </li>
                    <li>
                        Propriet√† 2: sappiamo che $p^{(k)} = r^{(k)}+\beta_k p^{(k+1)}$ per $k\geq 1$ quindi:
                        $${p^{(k)}}^T r^{(k)} = {r^{(k)}}^T r^{(k)} =(r^{(k)}+\beta_k p^{(k-1)})^T r^{(k)}- {r^{(k)}}^T r^{(k)} + \beta_k {p^{(k-1)}}^T r^{(k)} = {r^{(k)}}^Tr^{(k)}$$
                    </li>
                    <li>
                        Propriet√† 3: Dalla formula del residuo, si pu√≤ dedurre che $$r^{(k+1)} = r^{(k)} - \alpha^k A p^{(k)}\Ra Ap^{(k)} = \frac{r^{(k)}-r^{(k+1)}}{\alpha^{(k)}}$$
                    </li>
                    <li>
                        Propriet√† 4: Dimostriamo che ${r^{(k+1)}}^T r^{(k)}=0$
                        $${r^{(k+1)}}^Tr^{(k)} = (r^{(k)}-\alpha^{(k)}Ap^{(k)})^T r^{(k)} = {r^{(k)}}^Tr^{(k)} - \alpha^{(k)}{p^{(k)}}^T A r^{(k)}=$$ $$= {r^{(k)}}^T r^{(k)} - \alpha^{(k)}{p^{(k)}}^TA(p^{(k)}-\beta^{(k)}p^{(k-1)}) = {r^{(k)}}^T r^{(k)}-\alpha^{(k)}{p^{(k)}}^TAp^{(k)} + \alpha^{(k)}+\alpha^{(k)} \beta^{(k)}{p^{(k)}}^T Ap^{(k-1)} =$$ $$ = {r^{(k)}}^T r^{(k)} - \frac{{p^{(k)}}^Tr^{(k)}}{{p^{(k)}}^TAp^{(k)}}{p^{(k)}}^TAp^{(k)} $$ Per la propriet√† $2$ $${r^{(k)}}^Tr^{(k)} - {r^{(k)}}^Tr^{(k)} = 0$$

                        ( da qualche parte ho usato la propriet√† stellina ma non so dove.)
                    </li>
                </ol>
                Usando le varie propriet√†, ora posso semplificare le formule per $\alpha$ e $\beta$:
                <p>
                    <b>Step 1</b> risultato: $$\alpha_k = \frac{{p^{(k)}}^Tr^{(k)}}{{p^{(k)}}^TAp^{(k)}}$$Usando l'uguaglianza $Ap^{(k)}= v^{(k)}$ e la propriet√† $2$ otteniamo $$\frac{{r^{(k)}}^Tr^{(k)}}{{p^{(k)}}^Tv^{(k)}} = \frac{||r^{(k)}||_2^2}{{p^{(k)}}^Tv^{(k)}}$$
                </p>
                <p>
                    <b>Step 3</b> risultato: $$\beta_k = -\frac{{r^{(k)}}^T A p^{(k-1)}}{{p^{(k+1)}}^TAp^{(k-1)}} = \frac{{r^{(k)}}^TAp^{(k-1)}}{{p^{(k+1)}}^TAp^{(k-1)}} = \frac{{r^{(k)}}^TAp^{(k-1)}}{1}\cdot \frac{1}{{p^{(k+1)}}^TAp^{(k-1)}}$$
                    Dove per la propriet√† 3 abbiamo che $$\frac{-{r^{(k)}}^T(r^{(k-1)}-r^{(k)})}{\alpha^{(k-1)}}\cdot \frac{\alpha^{(k-1)}}{{p^{(k-1)}}^T(r^{(k-1)}-r^{(k)})}= -\frac{{r^{(k)}}^Tr^{(k-1)}-{r^{(k)}}^Tr^{(k)}}{{p^{(k-1)}}^Tr^{(k-1)}-p^{(k-1)}r^{(k)}}$$ che per la propriet√† $4$ e $1$ otteniamo che $$= \frac{||r^{(k)}||_2^2}{||r^{}(k-1)||_2^2}=\beta_k = {r^{(k-1)}}^Tr^{(k-1)}$$ per propriet√† 2
                </p>

                <p>
                    Mentre lo step $4$ vogliamo ora sapere in quante iterazioni converge il metodo. Se le direzioni $p^{(k)}$ scelte fossero tutte coniugate tra loro, allora sapremmo che, in aritmetica esatta, il metodo converge esattamente in $n$ iterazioni, che √® anche il numero massimo di iterazioni che ha senso fare, poi non si possono pi√π trovare iterazioni coniugate perch√® ad ogni passo il metodo confina il residuo a stare in uno spazio di dimensioni sempre pi√π piccole finch√® dopo $n$ passi $r^{(k)}$ star√† in uno spazio nullo. Basterebbero $n$ iterazioni perch√® quando si hanno $n$ iterazioni coniugate si pu√≤ sempre scrivere la soluzione del sistema. Per questa ragione della finitezza del numero di iterazioni il metodo del gradiente coniugato si potrebbe considerare un metodo diretto. <b>PER MATRICI SPARSE, QUESTO √à IL MIGLIOR METODO.</b>
                </p>
                

            </p>

            
        </section>
    </article>

    <!-- KaTeX and auto-render -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>

    <script>
    document.addEventListener("DOMContentLoaded", function() {
        const macros = {
            "\\CC": "\\mathbb{C}",
            "\\RR": "\\mathbb{R}",
            "\\QQ": "\\mathbb{Q}",
            "\\ZZ": "\\mathbb{Z}",
            "\\NN": "\\mathbb{N}",
            "\\EE": "\\mathrm{e}",
            "\\ii": "\\mathrm{i}",
            "\\dd": "\\mathrm{d}",
            "\\La": "\\Leftarrow",
            "\\Ra": "\\Longrightarrow",
            "\\sse": "\\Longleftrightarrow",
            "\\inv": "^{-1}"
        };

        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "\\[", right: "\\]", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false}
            ],
            macros: macros,
            ignoredTags: ["script", "noscript", "style", "textarea", "pre", "code"]
        });
    });
    </script>
     <script src="../theme.js"></script>
</body>
</html>